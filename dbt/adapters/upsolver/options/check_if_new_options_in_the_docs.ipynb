{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3a4e74a",
   "metadata": {},
   "source": [
    "## Import adapter options "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e8343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from connection_options import Connection_options\n",
    "from copy_options import Copy_options\n",
    "from target_options import Target_options\n",
    "from transformation_options import Transformation_options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4022ab27",
   "metadata": {},
   "source": [
    "## Define helpers to get new options lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d8508af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_docs(docs):\n",
    "    return list(map(str.strip, docs.lower().split('\\n')))\n",
    "\n",
    "def get_new_options(adapter, docs, options_type):\n",
    "    print(f'Check {options_type}:')\n",
    "    docs = prepare_docs(docs)\n",
    "    intersection = set(docs).intersection(adapter)\n",
    "    #check if all options in the adapter are still in the docs\n",
    "    if len(intersection) == len(adapter):\n",
    "        print('All options from the adapter are still in the documentation')\n",
    "    else:\n",
    "        print('!!! Not all options from the adapter are still in the documentation')\n",
    "    new_options = list(set(docs) - set(intersection))\n",
    "    if new_options:\n",
    "        print(f'New options to add: {new_options}')\n",
    "    else:\n",
    "        print(\"No new options\")\n",
    "        \n",
    "def get_new_source(adapter, docs):\n",
    "    \n",
    "    adapter_scources = list(adapter.keys())\n",
    "    docs_scources = list(docs.keys())\n",
    "    inter = set(adapter_scources).intersection(docs_scources)\n",
    " \n",
    "    new_sources = list(set(docs_scources) - set(inter))\n",
    "    if new_sources:\n",
    "        print(f'New sources to add: {new_sources}')\n",
    "    else:\n",
    "        print(\"No new source detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb8330e",
   "metadata": {},
   "source": [
    "   ## Paste Copy options from upsolver docs\n",
    "   ### Copy all options from:\n",
    "   #### https://docs.upsolver.com/sqlake/sql-command-reference/sql-jobs/create-job\n",
    "   ### 23.08.2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7216be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_copy_options = {\n",
    "    \n",
    "    \"s3\": \"\"\"COMMIT_INTERVAL\n",
    "             COMPRESSION\n",
    "             CONTENT_TYPE\n",
    "             DATE_PATTERN\n",
    "             DEDUPLICATE_WITH\n",
    "             DELETE_FILES_AFTER_LOAD\n",
    "             FILE_PATTERN\n",
    "             INITIAL_LOAD_PATTERN\n",
    "             INITIAL_LOAD_PREFIX\n",
    "             RUN_PARALLELISM\n",
    "             SKIP_ALL_VALIDATIONS\n",
    "             SKIP_VALIDATIONS\n",
    "             START_FROM\n",
    "             COLUMN_TRANSFORMATIONS\n",
    "             COMMENT\n",
    "             COMPUTE_CLUSTER\n",
    "             END_AT\n",
    "             EXCLUDE_COLUMNS\"\"\",\n",
    "\n",
    "    \"kafka\": \"\"\"COMMIT_INTERVAL\n",
    "                COMPRESSION\n",
    "                CONSUMER_PROPERTIES\n",
    "                CONTENT_TYPE\n",
    "                DEDUPLICATE_WITH\n",
    "                READER_SHARDS\n",
    "                RUN_PARALLELISM\n",
    "                START_FROM\n",
    "                STORE_RAW_DATA\n",
    "                COLUMN_TRANSFORMATIONS \n",
    "                COMMENT \n",
    "                COMPUTE_CLUSTER \n",
    "                END_AT \n",
    "                EXCLUDE_COLUMNS\"\"\",\n",
    "\n",
    "    \"mysql\": \"\"\"DDL_FILTERS\n",
    "                SKIP_SNAPSHOTS\n",
    "                SNAPSHOT_PARALLELISM\n",
    "                COLUMN_TRANSFORMATIONS \n",
    "                COMMENT \n",
    "                COMPUTE_CLUSTER \n",
    "                END_AT\n",
    "                EXCLUDE_COLUMNS\"\"\",\n",
    "\n",
    "    \"postgres\": \"\"\"HEARTBEAT_TABLE\n",
    "                   PARSE_JSON_COLUMNS\n",
    "                   PUBLICATION_NAME\n",
    "                   SKIP_SNAPSHOTS\n",
    "                   SNAPSHOT_PARALLELISM\n",
    "                   START_FROM\n",
    "                   COLUMN_TRANSFORMATIONS \n",
    "                   COMMENT \n",
    "                   COMPUTE_CLUSTER \n",
    "                   END_AT \n",
    "                   EXCLUDE_COLUMNS\"\"\",\n",
    "\n",
    "    \"kinesis\": \"\"\"COMMIT_INTERVAL\n",
    "                  COMPRESSION\n",
    "                  CONTENT_TYPE\n",
    "                  DEDUPLICATE_WITH\n",
    "                  READER_SHARDS\n",
    "                  RUN_PARALLELISM\n",
    "                  SKIP_ALL_VALIDATIONS\n",
    "                  SKIP_VALIDATIONS\n",
    "                  STORE_RAW_DATA\n",
    "                  START_FROM\n",
    "                  COLUMN_TRANSFORMATIONS\n",
    "                  COMMENT \n",
    "                  COMPUTE_CLUSTER\n",
    "                  END_AT \n",
    "                  EXCLUDE_COLUMNS\"\"\",\n",
    "    \n",
    "    \"mongodb\": \"\"\"SKIP_SNAPSHOTS\n",
    "                  SNAPSHOT_PARALLELISM\n",
    "                  START_FROM\n",
    "                  COLUMN_TRANSFORMATIONS \n",
    "                  COMMENT \n",
    "                  COMPUTE_CLUSTER \n",
    "                  END_AT \n",
    "                  EXCLUDE_COLUMNS\"\"\",\n",
    "    \n",
    "    \"mssql\": \"\"\"PARSE_JSON_COLUMNS\n",
    "                SKIP_SNAPSHOTS\n",
    "                SNAPSHOT_PARALLELISM\n",
    "                START_FROM\n",
    "                EXCLUDE_COLUMNS\n",
    "                COLUMN_TRANSFORMATIONS \n",
    "                COMMENT \n",
    "                COMPUTE_CLUSTER \n",
    "                END_AT \"\"\"\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e24578",
   "metadata": {},
   "source": [
    "## Get new create job sources and new copy options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d475eb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New sources to add: ['mssql', 'mongodb']\n"
     ]
    }
   ],
   "source": [
    "get_new_source(Copy_options, docs_copy_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c4de5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Copy kafka:\n",
      "All options from the adapter are still in the documentation\n",
      "New options to add: ['commit_interval', 'column_transformations']\n",
      "***************\n",
      "Check Copy mysql:\n",
      "All options from the adapter are still in the documentation\n",
      "New options to add: ['ddl_filters', 'snapshot_parallelism']\n",
      "***************\n",
      "Check Copy postgres:\n",
      "All options from the adapter are still in the documentation\n",
      "New options to add: ['snapshot_parallelism']\n",
      "***************\n",
      "Check Copy s3:\n",
      "All options from the adapter are still in the documentation\n",
      "New options to add: ['commit_interval', 'skip_validations', 'skip_all_validations']\n",
      "***************\n",
      "Check Copy kinesis:\n",
      "All options from the adapter are still in the documentation\n",
      "New options to add: ['commit_interval', 'skip_validations', 'skip_all_validations']\n",
      "***************\n"
     ]
    }
   ],
   "source": [
    "for source in Copy_options.keys():\n",
    "    get_new_options(Copy_options[source]['job_options'].keys(), docs_copy_options[source], f\"Copy {source}\")\n",
    "    print('***************')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf2c1f4",
   "metadata": {},
   "source": [
    "  ## Paste Connections options from upsolver docs\n",
    "  ### Copy all options from:\n",
    "  #### https://docs.upsolver.com/sqlake/sql-command-reference/sql-connections/create-connection\n",
    "  ### 23.08.2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d502b791",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_connections_options = {\n",
    "    \"s3\": \"\"\"AWS_ROLE\n",
    "             EXTERNAL_ID\n",
    "             AWS_ACCESS_KEY_ID\n",
    "             AWS_SECRET_ACCESS_KEY\n",
    "             PATH_DISPLAY_FILTER\n",
    "             PATH_DISPLAY_FILTERS\n",
    "             READ_ONLY\n",
    "             ENCRYPTION_KMS_KEY\n",
    "             ENCRYPTION_CUSTOMER_MANAGED_KEY\n",
    "             COMMENT\"\"\",\n",
    "\n",
    "    \"kafka\": \"\"\"HOST\n",
    "                HOSTS\n",
    "                CONSUMER_PROPERTIES\n",
    "                VERSION\n",
    "                REQUIRE_STATIC_IP\n",
    "                SSL\n",
    "                TOPIC_DISPLAY_FILTER\n",
    "                TOPIC_DISPLAY_FILTERS\n",
    "                COMMENT\"\"\",\n",
    "\n",
    "    \"glue_catalog\": \"\"\"AWS_ROLE\n",
    "                 EXTERNAL_ID\n",
    "                 AWS_ACCESS_KEY_ID\n",
    "                 AWS_SECRET_ACCESS_KEY\n",
    "                 DEFAULT_STORAGE_CONNECTION\n",
    "                 DEFAULT_STORAGE_LOCATION\n",
    "                 REGION\n",
    "                 DATABASE_DISPLAY_FILTER\n",
    "                 DATABASE_DISPLAY_FILTERS\n",
    "                 COMMENT\"\"\",\n",
    "\n",
    "    \"kinesis\": \"\"\"AWS_ROLE\n",
    "                  EXTERNAL_ID\n",
    "                  AWS_ACCESS_KEY_ID\n",
    "                  AWS_SECRET_ACCESS_KEY\n",
    "                  REGION\n",
    "                  READ_ONLY\n",
    "                  MAX_WRITERS\n",
    "                  STREAM_DISPLAY_FILTER\n",
    "                  STREAM_DISPLAY_FILTERS\n",
    "                  COMMENT\"\"\",\n",
    "\n",
    "    \"snowflake\":\"\"\"CONNECTION_STRING\n",
    "                   USER_NAME\n",
    "                   PASSWORD\n",
    "                   MAX_CONCURRENT_CONNECTIONS\n",
    "                   COMMENT\"\"\",\n",
    "\n",
    "    \"redshift\": \"\"\"CONNECTION_STRING\n",
    "                   USER_NAME\n",
    "                   PASSWORD\n",
    "                   MAX_CONCURRENT_CONNECTIONS\n",
    "                   COMMENT\"\"\",\n",
    "\n",
    "    \"mysql\": \"\"\"CONNECTION_STRING\n",
    "                USER_NAME\n",
    "                PASSWORD\n",
    "                COMMENT\"\"\",\n",
    "\n",
    "    \"postgres\": \"\"\"CONNECTION_STRING\n",
    "                   USER_NAME\n",
    "                   PASSWORD\n",
    "                   COMMENT\"\"\",\n",
    "\n",
    "    \"elasticsearch\": \"\"\"CONNECTION_STRING\n",
    "                        USER_NAME\n",
    "                        PASSWORD\n",
    "                        COMMENT\"\"\",\n",
    "\n",
    "    \"mongodb\": \"\"\"CONNECTION_STRING\n",
    "                  USER_NAME\n",
    "                  PASSWORD\n",
    "                  TIMEOUT\n",
    "                  COMMENT\"\"\",\n",
    "\n",
    "    \"mssql\": \"\"\"CONNECTION_STRING\n",
    "                USER_NAME\n",
    "                PASSWORD\n",
    "                COMMENT\"\"\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138a0a83",
   "metadata": {},
   "source": [
    "## Get new connection sources and new connection options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1802f784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New sources to add: ['mssql', 'mongodb']\n"
     ]
    }
   ],
   "source": [
    "get_new_source(Connection_options, docs_connections_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a96ab5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Connection s3:\n",
      "All options from the adapter are still in the documentation\n",
      "No new options\n",
      "***************\n",
      "Check Connection kafka:\n",
      "All options from the adapter are still in the documentation\n",
      "No new options\n",
      "***************\n",
      "Check Connection glue_catalog:\n",
      "All options from the adapter are still in the documentation\n",
      "No new options\n",
      "***************\n",
      "Check Connection kinesis:\n",
      "All options from the adapter are still in the documentation\n",
      "No new options\n",
      "***************\n",
      "Check Connection snowflake:\n",
      "All options from the adapter are still in the documentation\n",
      "No new options\n",
      "***************\n",
      "Check Connection redshift:\n",
      "All options from the adapter are still in the documentation\n",
      "No new options\n",
      "***************\n",
      "Check Connection mysql:\n",
      "All options from the adapter are still in the documentation\n",
      "No new options\n",
      "***************\n",
      "Check Connection postgres:\n",
      "All options from the adapter are still in the documentation\n",
      "No new options\n",
      "***************\n",
      "Check Connection elasticsearch:\n",
      "All options from the adapter are still in the documentation\n",
      "No new options\n",
      "***************\n"
     ]
    }
   ],
   "source": [
    "for source in Connection_options.keys():\n",
    "    get_new_options(Connection_options[source].keys(), docs_connections_options[source], f\"Connection {source}\")\n",
    "    print('***************')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e479c8",
   "metadata": {},
   "source": [
    " ## Paste Transformation options from upsolver docs\n",
    "   ### Copy all options from:\n",
    "   #### https://docs.upsolver.com/sqlake/sql-command-reference/sql-jobs/create-job/sql-transformation-jobs/merge#job_options\n",
    "   ### 23.08.2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "daad5d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_transformation_options = {\n",
    "    \"s3\": \"\"\"COMPRESSION\n",
    "             DATE_PATTERN\n",
    "             FILE_FORMAT\n",
    "             OUTPUT_OFFSET\n",
    "             AGGREGATION_PARALLELISM\n",
    "             ALLOW_CARTESIAN_PRODUCTS\n",
    "             COMMENT\n",
    "             COMPUTE_CLUSTER\n",
    "             END_AT\n",
    "             RUN_INTERVAL\n",
    "             RUN_PARALLELISM\n",
    "             START_FROM\"\"\",\n",
    "    \n",
    "    \"elasticsearch\": \"\"\"BULK_MAX_SIZE_BYTES\n",
    "                        INDEX_PARTITION_SIZE\n",
    "                        ROUTING_FIELD_NAME\n",
    "                        AGGREGATION_PARALLELISM\n",
    "                        ALLOW_CARTESIAN_PRODUCTS\n",
    "                        COMMENT\n",
    "                        COMPUTE_CLUSTER\n",
    "                        END_AT\n",
    "                        RUN_INTERVAL\n",
    "                        RUN_PARALLELISM\n",
    "                        START_FROM\"\"\",\n",
    "    \n",
    "    \"snowflake\": \"\"\"ADD_MISSING_COLUMNS\n",
    "                    COMMIT_INTERVAL\n",
    "                    CUSTOM_INSERT_EXPRESSIONS\n",
    "                    CUSTOM_UPDATE_EXPRESSIONS\n",
    "                    KEEP_EXISTING_VALUES_WHEN_NULL\n",
    "                    AGGREGATION_PARALLELISM\n",
    "                    ALLOW_CARTESIAN_PRODUCTS\n",
    "                    COMMENT\n",
    "                    COMPUTE_CLUSTER\n",
    "                    END_AT\n",
    "                    RUN_INTERVAL\n",
    "                    RUN_PARALLELISM\n",
    "                    START_FROM\"\"\",\n",
    "    \n",
    "    \"datalake\": \"\"\"ADD_MISSING_COLUMNS\n",
    "                   AGGREGATION_PARALLELISM\n",
    "                   ALLOW_CARTESIAN_PRODUCTS\n",
    "                   COMMENT\n",
    "                   COMPUTE_CLUSTER\n",
    "                   END_AT\n",
    "                   RUN_INTERVAL\n",
    "                   RUN_PARALLELISM\n",
    "                   START_FROM\"\"\",\n",
    "    \n",
    "    \"redshift\": \"\"\"FAIL_ON_WRITE_ERROR\n",
    "                   SKIP_FAILED_FILES\n",
    "                   AGGREGATION_PARALLELISM\n",
    "                   ALLOW_CARTESIAN_PRODUCTS\n",
    "                   COMMENT\n",
    "                   COMPUTE_CLUSTER\n",
    "                   END_AT\n",
    "                   RUN_INTERVAL\n",
    "                   RUN_PARALLELISM\n",
    "                   START_FROM\"\"\",\n",
    "    \n",
    "    \"postgres\": \"\"\"AGGREGATION_PARALLELISM\n",
    "                     ALLOW_CARTESIAN_PRODUCTS\n",
    "                     COMMENT\n",
    "                     COMPUTE_CLUSTER\n",
    "                     END_AT\n",
    "                     RUN_INTERVAL\n",
    "                     RUN_PARALLELISM\n",
    "                     START_FROM\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9749749",
   "metadata": {},
   "source": [
    "## Get new transformation sources and new transformation options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b603aa9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New sources to add: ['postgres']\n"
     ]
    }
   ],
   "source": [
    "get_new_source(Transformation_options, docs_transformation_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d0aecc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Transformation s3:\n",
      "!!! Not all options from the adapter are still in the documentation\n",
      "No new options\n",
      "***************\n",
      "Check Transformation elasticsearch:\n",
      "All options from the adapter are still in the documentation\n",
      "New options to add: ['routing_field_name']\n",
      "***************\n",
      "Check Transformation snowflake:\n",
      "All options from the adapter are still in the documentation\n",
      "New options to add: ['commit_interval']\n",
      "***************\n",
      "Check Transformation datalake:\n",
      "All options from the adapter are still in the documentation\n",
      "No new options\n",
      "***************\n",
      "Check Transformation redshift:\n",
      "All options from the adapter are still in the documentation\n",
      "No new options\n",
      "***************\n"
     ]
    }
   ],
   "source": [
    "for source in Transformation_options.keys():\n",
    "    get_new_options(Transformation_options[source].keys(), docs_transformation_options[source], f\"Transformation {source}\")\n",
    "    print('***************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c29c55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
